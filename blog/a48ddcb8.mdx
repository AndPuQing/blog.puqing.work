---
authors:
- PuQing
date: 2024-08-07 17:50
keywords:
- 随机过程
- 概率论
- 机器学习
tags:
- 随机过程
- 概率论
- 机器学习
---
# 高斯过程

在 [布朗运动与朗之万方程](./265f8ac6) 中已经介绍过随机过程，而高斯过程 (**Gaussian process**) 是一个特殊的随机过程。在高斯过程中，连续输入空间中每个点都是与一个正态分布的随机变量相关联 [^1]。

从单变量高斯分布说起。在 [单变量高斯分布](./971deee9#%E5%8D%95%E5%8F%98%E9%87%8F%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83) 中我们已经写出了单变量高斯分布的公式，在这里重复一遍。


<!--truncate-->
## 单变量高斯分布

单变量高斯分布的概率密度函数为

$$
p(x) = \frac{1}{\sigma\sqrt{2\pi}}\mathrm{exp}(-\frac{(x-\mu)^2}{2\sigma^2})
$$
其中 $\mu$ 和 $\sigma$ 分别表示均值和方差，这个概率密度函数曲线画出来就是我们熟悉的钟形曲线，均值和方差唯一地决定了曲线的形状。

## 多元高斯分布

从一元高斯分布推广到多元高斯分布，假设各维度之间相互独立。

$$
p\left(x_{1}, x_{2}, \ldots, x_{n}\right)=\prod_{i=1}^{n} p\left(x_{i}\right)=\frac{1}{(2 \pi)^{\frac{n}{2}} \sigma_{1} \sigma_{2} \ldots \sigma_{n}} \exp \left(-\frac{1}{2}\left[\frac{\left(x_{1}-\mu_{1}\right)^{2}}{\sigma_{1}^{2}}+\frac{\left(x_{2}-\mu_{2}\right)^{2}}{\sigma_{2}^{2}}+\ldots+\frac{\left(x_{n}-\mu_{n}\right)^{2}}{\sigma_{n}^{2}}\right]\right)
$$
其中 $\mu_{1},\mu_{2},\dots$ 和 $\sigma_{1},\sigma_{2},\dots$ 分别是第 1 维、第二维 … 的均值和方差。用向量和矩阵表示上式，令

$$
\boldsymbol{x}-\boldsymbol{\mu}=\left[x_{1}-\mu_{1}, x_{2}-\mu_{2}, \ldots x_{n}-\mu_{n}\right]^{T}
$$
$$
K=\begin{bmatrix}
\sigma_{1}^{2} & 0 & \cdots & 0 \\
0 & \sigma_{2}^{2} & \cdots & 0 \\
\vdots & \vdots & \ddots & 0 \\
0 & 0 & 0 & \sigma_{n}^{2}
\end{bmatrix}
$$
则

$$
\sigma_{1} \sigma_{2} \ldots \sigma_{n}=|K|^{\frac{1}{2}}
$$
$$
\frac{\left(x_{1}-\mu_{1}\right)^{2}}{\sigma_{1}^{2}}+\frac{\left(x_{2}-\mu_{2}\right)^{2}}{\sigma_{2}^{2}}+\ldots+\frac{\left(x_{n}-\mu_{n}\right)^{2}}{\sigma_{n}^{2}}=(x-\mu)^{T} K^{-1}(x-\mu)
$$
代入公式可得：

$$
p(\boldsymbol{x}) = (2\pi)^{-\frac{n}{2}}|K|^{-\frac{1}{2}}\mathrm{exp}\left( -\frac{1}{2}(\boldsymbol{x-\mu})^TK^{-1}(\boldsymbol{x-\mu}) \right)
$$
其中 $\boldsymbol{\mu} \in \mathbb{R}^n$ 是均值向量， $K \in \mathbb{R}^{n \times n}$ 为协方差矩阵，由于我们假设了各维度直接相互独立，因此 $K$ 是一个对角矩阵。在各维度变量相关时，上式的形式仍然一致，但此时协方差矩阵 $K$ 不再是对角矩阵，可以称为 [Gramian矩阵](./4be8258f)。上式通常也简写为

$$
x \sim \mathcal{N}(\boldsymbol{\mu}, K)
$$
## 无限元高斯分布

高斯过程则是更进一步，它一个定义在连续域上的无限多个高斯随机变量所组成的随机过程；

对于一个连续域 $T$(假设是一个时间轴)，如果我们在连续域上任选 $n$ 个时刻：$t_{1},t_{2},\dots,t_{n} \in T$ ，使得获得的一个 $n$ 维向量 $\left\{\xi_{1}, \xi_{2}, \xi_{3}, \ldots, \xi_{n}\right\}$ 都满足一个 $n$ 维高斯分布，那么这个 $\left\{ \xi_{t} \right\}$ 就是一个高斯过程。

对于一个 $p$ 维的高斯分布而言，决定他的分布是两个参数，一个是 $p$ 维的均值向量 $\mu_{p}$ ，他反映了 $p$ 维高斯分布中每一维随机变量的期望，另一个就是 $p \times p$ 的协方差矩阵 $\Sigma_{p \times p}$ ，他反映了高维分布中，每一维自身的方差，以及不同维度之间的协方差。

定义在连续域 $T$ 上的高斯过程是一样，它是无限维的高斯分布，他同样需要描述每一个时间点 $t$ 上的均值，但是这个时候就不能用向量了，因为是在连续域上的，维数是无限的，因此就应该定义成一个关于时刻 $t$ 的函数：$m(t)$。

协方差矩阵也是同理, 无限维的情况下就定义为一个核函数 $k(s, t)$ ，其中 $s$ 和 $t$ 表示任意两个时刻，核函数也称协方差函数。核函数是一个高斯过程的核心，它决定了高斯过程的性质，在研究和实践中，核函数有很多种不同的类型，他们对高斯过程的衡量方法也不尽相同，这里我们介绍和使用最为常见的一个核函数：径向基函数 $\mathrm{RBF}$，其定义如下：

$$
k(s, t)=\sigma^{2} \exp \left(-\frac{\|s-t\|^{2}}{2 l^{2}}\right)
$$
这里面的 $\sigma$ 和 $l$ 是径向基函数的超参数， $s$ 和 $t$ 表示高斯过程连续域上的两个不同的时间点, $\|s-t\|^{2}$ 是一个二范式，径向基函数输出的是一个标量，它代表的就是 $s$ 和 $t$ 两个时间点各自所代表的高斯分布之间的协方差值，很明显径向基函数是一个关于 $s,t$ 距离负相关的函数，两个点距离越大，两个分布之间的协方差值越小，即相关性越小，反之越靠近的两个时间点对应的分布其协方差值就越大。

整个高斯过程就可以确定为： $\xi_{t} \sim G P(m(t), k(s, t))$

## 机器学习下的高斯过程

在机器学习下，我们想要的是在观测到一些样本对下，去更新高斯过程的均值函数和核函数。

那假设我们有已知数据集 $\mathcal{D}=\left\{ (t_{1},\xi_{1}), (t_{2},\xi_{2}) ,(t_{3},\xi_{3})),\dots,(t_{n},\xi_{n})\right\}$，并且有时间向量 $\boldsymbol{t}$，以及采样值向量 $\boldsymbol{\xi}$

那么余下的所有非观测点，在连续域上我们定义为 $t$，值为 $\xi_{t}$；那么，其联合分布显然满足无限维高斯分布。

$$
\begin{bmatrix}
\boldsymbol{\xi} \\
\xi_{t}
\end{bmatrix}\sim
\mathrm{GP}\left( \begin{bmatrix}
\boldsymbol{\xi} \\
m(t)
\end{bmatrix},\begin{bmatrix}
k(\boldsymbol{t},\boldsymbol{t}) & k(\boldsymbol{t},t) \\
k(t,\boldsymbol{t}) & k(t,t)
\end{bmatrix} \right) 
$$
于是条件概率就是，

$$
\xi_{t}|\boldsymbol{\xi} \sim \mathrm{GP}\left(m^*(t),k^*(s,t)  \right) 
$$
其中

$$
\begin{cases}
m^*(t) = k(t,\boldsymbol{t})k(\boldsymbol{t},\boldsymbol{t})^{-1}(\boldsymbol{\xi}-\mu(\boldsymbol{\xi}))+ m(t) \\
k^*(s,t) = k(s,t)- k(t,\boldsymbol{t})k(\boldsymbol{t},\boldsymbol{t})^{-1}k(\boldsymbol{t},t)
\end{cases}
$$
:::tip[]
这个地方把各个量的 shape 标出来

$$
\begin{cases}
k(t,\boldsymbol{t}) \in \mathbb{R}^{1\times n} \\
k(\boldsymbol{t},\boldsymbol{t}) \in \mathbb{R}^{n\times n}  \\
k(\boldsymbol{t},t) \in \mathbb{R}^{n \times 1} 
\end{cases} 
$$
:::
其中 $\mu(\boldsymbol{\xi})$ 是给定数据集的均值。

从公式中我们可以看到，协方差 $k^*$ 减掉的后面的那一项实际上表示了观测到数据后函数分布不确定性的减少，如果第二项非常接近于 0，说明观测数据后我们的不确定性几乎不变，反之如果第二项非常大，则说明不确定性降低了很多 [^2][^3][^4]。

[^1]:[高斯过程 - 维基百科，自由的百科全书](https://zh.wikipedia.org/wiki/%E9%AB%98%E6%96%AF%E8%BF%87%E7%A8%8B)
[^2]:[GitHub-wegatron/gaussian_process_ground_seg](https://github.com/wegatron/gaussian_process_ground_seg)
[^3]:[RW.pdf](https://gaussianprocess.org/gpml/chapters/RW.pdf#page=25.83)
[^4]:[zhihu.com/question/46631426](https://www.zhihu.com/question/46631426)
