---
authors:
- PuQing
date: 2023-08-25 20:08
keywords:
- 机器学习
- 正则化
tags:
- 机器学习
- 正则化
---
# 重谈 L1 与 L2 正则

L1 与 L2 的比较是一个老生常谈的问题

本文章想要回答下面这个问题

:::info
前文提到 $L_{1}$、$L_{2}$ 正则都是想要降低模型的复杂度 (权重趋近 $0$)，那么二者是否有倾重？或者说区别是什么？


:::
## L1- 正则化更稀疏

为了探究 $L_{1}$ 和 $L_{2}$ 正则化效果，设计了一个蒙特卡洛实验。


<!--truncate-->
我们知道机器学习就是寻找使得 " 经验 " 损失函数 $L(x;\theta)$ 最小的参数 $\theta_{0}$ 的过程，对于给定的训练集 $\symcal{D}$，损失函数可构成参数空间。

对于给定的参数 $w,h$，以及损失函数 ${L}$，那么该损失函数需要满足如下条件：

:::tip[]

1. 损失函数 $L$ 一阶导连续，且在最优点 $(w_{0},h_{0})$ 一阶偏导分别为零。
2. 损失函数 $L$ 二阶导连续，且海赛矩阵正定。

:::
不妨可以取

$$
L=a(w-w_{0})^2+b(h-h_{0})^2+c(w-w_{0})(h-h_{0})
$$
其中 $a>0,b>0$

:::note[]
在本次模拟中，我们对参数分别采用下面采样方式：

$$
a\sim U(0,10),b\sim U(0,10),c\sim(-2,2)
$$
而极值点我们下面分布中取到：

$$
w_{0}\sim(-10,10),h_{0}\sim(-10,10)
$$
:::
在某一参数 $a,b,c$ 下的参数空间

![image.png](https://images.puqing.work/2023/08/26/64e97b6a065af.png)

再来看结构风险，对于 $\mathrm{L_{1}},\mathrm{L_{2}}$ 正则项，在此案例中可以写为：

$$
\begin{array}{c}
L_1 = |w|+ |h| \\
L_2 = \sqrt{ w^2+h^2 } 
\end{array}
$$
并假定该模拟能优化结构风险至某一下确界 $\lambda$，即

$$
\begin{array}{c}
L_1 = |w|+ |h| =  \lambda  \\
L_2 = \sqrt{w^2+h^2} = \lambda  
\end{array}
$$
如果极值点位于上述确界 $\lambda$ 内，则结果会非常平凡，所以我们需要对模拟的极值点进行限制，

$$
\begin{array}{c}
L_1 = |w|+ |h| \ge  \lambda  \\
L_2 = \sqrt{w^2+h^2} \ge \lambda  
\end{array}
$$
如下图所示：
![output.png](https://images.puqing.work/2023/08/26/64e9827b7d78a.png)

之后我们检查在边界上的点在随机选取的极值点下使得经验风险最小的值
![Untitled.bmp](https://images.puqing.work/2023/08/26/64e9855d66c51.bmp)

之后我们再判断此时的参数 $w,h$ 是否其中一个趋近于 $0$。即：

$$
\mathrm{dist} = |w\times h |
$$
模拟化结果：

![image.png](https://images.puqing.work/2023/08/27/64eb33e69a948.png)

:::note[]
其中蓝色表示此处的随机损失函数，正则化后某一项系数趋近 $0$，红色表示随机函数正则后系数不会为 $0$ 的情况

:::
:::tip[ 结果分析]

- 正如上面模拟 (5000 次实验) 中所看到的，$\mathrm{L}_{1}$ 正则将顶点呈锥形区域的任意损失函数的某一系数正则为 $0$，而 $\mathrm{L}_{2}$ 正则项将靠近坐标轴附近的损失函数的某一系数才会正则为 $0$
  ![Untitled.bmp](https://images.puqing.work/2023/08/27/64eb3c4d73b55.bmp)
- 另外，我们注意到 $\mathrm{L}_{2}$ 正则项产生了很多 `临界` 零点 (即橙色，淡蓝色的点)，反观 $L_{1}$ 正则项主要是蓝色点以及红色点
- 显然，$L_{1}$ 正则项比 $L_{2}$ 正则项更容易产生零系数 ($74\%$ 比 $5\%$)

:::
## L1 和 L2 正则项确实能降低模型复杂度

上面损失函数的对于参数 $w,h$ 的重要性是相当的，是等同地位的，下面我们来看看如果我们降低其中一个特征的重要性 (即降低其中一个参数的系数) 模拟结果会怎么样。

:::note[ 采样设置]
此时对于系数的采样变为：

$$
\begin{cases}
50\%,\quad a\sim U(0,10),b=0.1 ,c=0\\
50\%,\quad a=0.1,b\sim U(0,10),c=0
\end{cases}
$$
:::
结果：
![image.png](https://images.puqing.work/2023/08/27/64eb404e19395.png)

:::tip[ 结果分析]

- 对于这种<Highlight>参数空间</Highlight>，会导致比一般的情况更多的零系数，这是符合预期的，但是对于 $\mathrm{L}_{1}$ 正则项来说，其效果并不大 ($74\%\to 81\%$)；
- 另外，$\mathrm{L}_{2}$ 的零系数大幅度增加 ($5\%\to 46\%$)

:::
对于这种一个特征预测性能较差的情况，我们肯定是需要更多的零系数。上面两种正则方式都可以实现。

:::danger[]
注意：这里我们只是对低维度情况下对于简单损失函数进行了模拟，对于高维度情况需要做更多的实验进行验证，但是我们有理由相信其结果与低维度下是一致的

:::
## 为什么 L1 更稀疏

:::quote[]
我们现在有实验数据表明，对于许多不同的损失函数 (至少是二元二次损失函数)，$L_{1}$ 往往比 $L_{2}$ 能够获得更多的零系数。但是究竟为什么会发生这种情况？

:::
为了回答这个问题，我们放大约束的区域。

:::info
因为最小位置在约束区域之外，所以我们知道正则化后的系数将位于约束边界的某处。我们需要找到在边界上使损失函数最低的点。


:::
::::tip[ $L_{1}$ 情况]
![Untitled.bmp](https://images.puqing.work/2023/08/27/64eb546c7eb7b.bmp)
我们认为最优位置 $(w_{0},h_{0})$ 就是菱形的尖端 (即红色点)

:::tip[ 证明：任何远离该点的移动都将增加损失。]
可以看到红色点 $(w_{0},h_{0})$ 所在的虚轮廓线。在该轮廓线上的所有位置都具有相同的损失值。而往点 $(w_{0},h_{0})$ 外侧任意位置都会有更高的损失值；

沿着菱形的边缘远离红色点 $(w_{0},h_{0})$ 的移动，都会增加损失（因为离开了虚轮廓线且向外），这就是为什么黄色的点都会有更高损失的原因

:::
::::
:::tip[ $L_{2}$ 情况]
![Untitled.bmp](https://images.puqing.work/2023/08/27/64eb50f35f6a1.bmp)
对于上图 $L_{2}$ 情况，我们认为最优位置是在新红点位置 $(w_{0},h_{0})$，而不是在坐标轴上。在点 $(w_{0},h_{0})$ 处一定存在虚轮廓线于其相切（请注意轮廓线数量是无限的）。基于与 $L_{1}$ 相同的原因，黄色点同样不是最优位置。

:::
对于这种情况下，$L_{1}$ 优化得到了零系数，而 $L_{2}$ 没有。损失函数相同，正则项值也一样。

::::quote[]
但是 $L_{2}$ 正则没有优点吗
![Untitled.bmp](https://images.puqing.work/2023/08/27/64eb5eb064e8e.bmp)

我们看上面案例，在这种情况下，对于 $L_{1}$ 正则，无论我们如何移动，损失都会立即上升，因为菱形的锐角会迫使其远离 " 经验 " 分析的最优点；对于 $L_{2}$ 正则，约束圆允许我们可以向上移动，离开轮廓线，并接近黑点。当损失函数最小值接经轴时，$L_{2}$ 黑点将接近轴，反之亦然。

:::tip[]
同时，在这里我们也一定程度上解释了上面模拟实验中 $L_{2}$ 正则中出现的一系列 `中间` 参数值

:::
::::
## 相关资料

- [The difference between L1 and L2 regularization](https://explained.ai/regularization/L1vsL2.html)

## 遗留问题

:::quote[]
第一：在这里我们通篇通过实验对 $L_{1}$ 正则项相比于 $L_{2}$ 更加稀疏进行了解释；但是在理论上或者是代数上并未对其完整的证明。

第二：$L_{2}$ 在梯度上因为有深刻的含义，这里没有更加充分的阐述

:::
